"""
This module provides classes for loading datasets generated by DataGenerator.
"""
import json
import math
import os
import random
from dataclasses import dataclass
from typing import Tuple

import geopandas as gpd
import numpy as np
import shapely
from shapely.geometry import LineString, Point, Polygon

from sarenv.utils.logging_setup import get_logger

log = get_logger()


@dataclass
class SARDatasetItem:
    """
    A dataclass to hold the data for a single generated SAR environment size.

    Attributes:
        size (str): The name of the size (e.g., 'small', 'medium').
        center_point (tuple[float, float]): The (longitude, latitude) of the dataset's center.
        radius_km (float): The radius used to generate this dataset item.
        bounds (tuple[float, float, float, float]): The projected bounds (minx, miny, maxx, maxy) of the data.
        features (gpd.GeoDataFrame): A GeoDataFrame containing all geographic features.
        heatmap (np.ndarray): A 2D NumPy array representing the probability heatmap.
    """
    size: str
    center_point: tuple[float, float]
    radius_km: float
    bounds: Tuple[float, float, float, float]
    features: gpd.GeoDataFrame
    heatmap: np.ndarray


class DatasetLoader:
    """
    Loads a master SAR environment dataset and dynamically clips it to various sizes.

    This class is designed to load the 'master' files created by the
    DataGenerator's export_dataset method and then process them on-the-fly.
    """

    def __init__(self, dataset_directory: str):
        """
        Initializes the DynamicDatasetLoader.

        Args:
            dataset_directory (str): The path to the directory containing the master dataset files.
        """
        if not os.path.isdir(dataset_directory):
            log.error(f"Dataset directory not found at: {dataset_directory}")
            raise FileNotFoundError(
                f"The specified directory does not exist: {dataset_directory}"
            )

        self.dataset_directory = dataset_directory
        self.master_features_path = os.path.join(
            self.dataset_directory, "features_master.geojson"
        )
        self.master_heatmap_path = os.path.join(
            self.dataset_directory, "heatmap_master.npy"
        )

        if not os.path.exists(self.master_features_path):
            raise FileNotFoundError(
                f"Master features file not found: {self.master_features_path}"
            )
        if not os.path.exists(self.master_heatmap_path):
            raise FileNotFoundError(
                f"Master heatmap file not found: {self.master_heatmap_path}"
            )

        self.size_radii_km = {
            "small": 0.6,
            "medium": 1.8,
            "large": 3.2,
            "xlarge": 9.9,
        }

        # Cache for lazy-loading the master data
        self._master_features_gdf = None
        self._master_features_gdf_proj = None
        self._master_heatmap = None
        self._center_point = None
        self._projected_crs = None
        self._meter_per_bin = None
        self._bounds = None
        # New: Cache for the combined probability map
        self._combined_master_map = None

    def _get_utm_epsg(self, lon: float, lat: float) -> str:
        """Calculates the appropriate UTM zone EPSG code for a given point."""
        zone = int((lon + 180) / 6) + 1
        return f"326{zone}" if lat >= 0 else f"327{zone}"

    def _generate_combined_master_map(self):
        """
        Generates a master probability map by combining a distance-based bell curve
        with the feature-based heatmap. This is executed once and cached.
        """
        log.info("Generating combined distance and feature probability master map...")

        # 1. Define the Bell Curve (2D Gaussian)
        radius_m_95p = self.size_radii_km["xlarge"] * 1000
        sigma = radius_m_95p / math.sqrt(-2 * math.log(0.05))
        log.info(f"Calculated Gaussian sigma: {sigma:.2f} meters for 95% containment at {radius_m_95p/1000} km.")

        # 2. Create coordinate grids for the master heatmap
        h, w = self._master_heatmap.shape
        master_minx, master_miny, _, _ = self._bounds
        
        y_indices, x_indices = np.mgrid[0:h, 0:w]
        x_coords_world = (x_indices * self._meter_per_bin) + master_minx
        y_coords_world = (y_indices * self._meter_per_bin) + master_miny

        center_point_gdf = gpd.GeoDataFrame(geometry=[shapely.Point(self._center_point)], crs="EPSG:4326")
        center_point_proj = center_point_gdf.to_crs(self._projected_crs)
        center_x_world = center_point_proj.geometry.x.iloc[0]
        center_y_world = center_point_proj.geometry.y.iloc[0]

        dist_sq = (x_coords_world - center_x_world)**2 + (y_coords_world - center_y_world)**2
        bell_curve_map = np.exp(-dist_sq / (2 * sigma**2))

        # 3. Normalize the original feature heatmap
        feature_heatmap_sum = np.sum(self._master_heatmap)
        if feature_heatmap_sum == 0:
            log.warning("Master feature heatmap is all zeros. Combined map will be based on distance only.")
            feature_prob_map = np.ones_like(self._master_heatmap)
        else:
            feature_prob_map = self._master_heatmap / feature_heatmap_sum
        
        # 4. Combine the probabilities and NORMALIZE the entire master map
        combined_map_unnormalized = bell_curve_map * feature_prob_map
        total_sum = np.sum(combined_map_unnormalized)
        if total_sum > 0:
            self._combined_master_map = combined_map_unnormalized / total_sum
        else:
            self._combined_master_map = combined_map_unnormalized # remains zeros
        
        log.info("Successfully generated and cached the normalized combined master map.")
        assert np.isclose(np.sum(self._combined_master_map), 1.0, atol=1e-6) or np.sum(self._combined_master_map) == 0


    def _load_master_data_if_needed(self):
        """Loads the master GeoJSON and heatmap from disk if not already in cache."""
        if self._combined_master_map is not None:
            return

        log.info("Loading master dataset from disk for the first time...")
        try:
            with open(self.master_features_path, "r") as f:
                geojson_data = json.load(f)

            # --- Load Metadata ---
            self._center_point = tuple(geojson_data["center_point"])
            self._meter_per_bin = geojson_data["meter_per_bin"]
            self._bounds = tuple(geojson_data["bounds"])
            log.info(
                f"Loaded metadata: center={self._center_point}, resolution={self._meter_per_bin} m/bin"
            )

            # --- Determine Projected CRS ---
            self._projected_crs = f"EPSG:{self._get_utm_epsg(self._center_point[0], self._center_point[1])}"

            # --- Load Main Data ---
            self._master_features_gdf = gpd.GeoDataFrame.from_features(
                geojson_data["features"], crs="EPSG:4326"
            )
            self._master_features_gdf_proj = self._master_features_gdf.to_crs(self._projected_crs)
            self._master_heatmap = np.load(self.master_heatmap_path)
            
            # New: Generate the combined map
            self._generate_combined_master_map()


        except KeyError as e:
            log.error(
                f"Could not find required key '{e}' in {self.master_features_path}. The file might be corrupt or incomplete."
            )
        except Exception as e:
            log.error(
                f"An error occurred while loading the master dataset: {e}",
                exc_info=True,
            )

    def get_available_sizes(self) -> list[str]:
        """Returns a list of all predefined size names."""
        return list(self.size_radii_km.keys())

    def _world_to_image_for_master(
        self, x_world: np.ndarray, y_world: np.ndarray
    ) -> tuple[np.ndarray, np.ndarray]:
        """Converts world coordinates (projected) to pixel coordinates of the master heatmap."""
        master_minx, master_miny, _, _ = self._bounds
        x_img = (x_world - master_minx) / self._meter_per_bin
        y_img = (y_world - master_miny) / self._meter_per_bin
        return x_img.astype(int), y_img.astype(int)

    def load_size(self, size: str) -> SARDatasetItem | None:
        """
        Loads the master data and clips it to a specified size. The resulting
        heatmap is a crop of the master probability map and its sum will reflect
        its portion of the total probability.

        Args:
            size (str): The name of the size to load (e.g., 'medium').

        Returns:
            SARDatasetItem | None: A dataclass instance containing the clipped/cropped data.
        """
        log.info(f"Attempting to generate dataset for size: '{size}'")
        if size not in self.size_radii_km:
            log.error(
                f"Size '{size}' is not a valid size name. Available sizes: {self.get_available_sizes()}"
            )
            return None

        self._load_master_data_if_needed()

        if self._combined_master_map is None:
            log.error("Cannot process size request because the combined master map failed to generate.")
            return None

        radius_km = self.size_radii_km[size]

        # Create clipping geometry for features
        clipping_point_wgs84 = gpd.GeoDataFrame(geometry=[shapely.Point(self._center_point)], crs="EPSG:4326")
        clipping_point_proj = clipping_point_wgs84.to_crs(self._projected_crs)
        clipping_circle_proj = clipping_point_proj.buffer(radius_km * 1000).iloc[0]
        clipped_bounds = clipping_circle_proj.bounds

        clipped_features_proj = gpd.clip(self._master_features_gdf_proj, clipping_circle_proj)
        log.info(f"Clipped features to {len(clipped_features_proj)} items for size '{size}'.")

        # Get pixel boundaries for cropping the heatmap
        min_x_w, min_y_w, max_x_w, max_y_w = clipped_bounds
        img_min_x, img_min_y = self._world_to_image_for_master(np.array([min_x_w]), np.array([min_y_w]))
        img_max_x, img_max_y = self._world_to_image_for_master(np.array([max_x_w]), np.array([max_y_w]))

        h, w = self._combined_master_map.shape
        img_min_y, img_max_y = np.clip([img_min_y[0], img_max_y[0]], 0, h)
        img_min_x, img_max_x = np.clip([img_min_x[0], img_max_x[0]], 0, w)

        # Crop the combined map
        cropped_map = self._combined_master_map[img_min_y:img_max_y, img_min_x:img_max_x]

        # Create a circular mask for the cropped area
        yy, xx = np.mgrid[img_min_y:img_max_y, img_min_x:img_max_x]
        center_x_master_px = (clipping_point_proj.geometry.x.values[0] - self._bounds[0]) / self._meter_per_bin
        center_y_master_px = (clipping_point_proj.geometry.y.values[0] - self._bounds[1]) / self._meter_per_bin
        radius_px = (radius_km * 1000) / self._meter_per_bin
        mask = ((xx - center_x_master_px)**2 + (yy - center_y_master_px)**2) <= radius_px**2

        # Apply mask. The resulting map is NOT re-normalized.
        final_heatmap = np.where(mask, cropped_map, 0)
        log.info(f"Final heatmap for size '{size}' has a probability sum of {np.sum(final_heatmap):.4f}")

        return SARDatasetItem(
            size=size,
            center_point=self._center_point,
            radius_km=radius_km,
            bounds=clipped_bounds,
            features=clipped_features_proj,
            heatmap=final_heatmap,
        )

    def load_all(self) -> dict[str, SARDatasetItem]:
        """
        Loads and processes data for all available sizes.

        Returns:
            dict[str, SARDatasetItem]: A dictionary where keys are size names
                                       and values are the corresponding SARDatasetItem objects.
        """
        log.info("Loading all available sizes from the master dataset.")
        all_data = {}
        for size in self.get_available_sizes():
            item = self.load_size(size)
            if item:
                all_data[size] = item
        return all_data



class SurvivorLocationGenerator:
    """
    Generates a plausible survivor location based on geographic features.

    The location is generated by first selecting a feature type (e.g., 'road',
    'building') weighted by the total area of influence of each type, then
    selecting a specific feature within that type, and finally generating a
    random point within a 30-meter buffer of that feature that is also inside
    the main search radius.
    """
    def __init__(self, dataset_item: SARDatasetItem):
        """
        Initializes the generator with a dataset item.

        Args:
            dataset_item (SARDatasetItem): The dataset item containing the features.
        """
        self.dataset_item = dataset_item
        self.features = dataset_item.features.copy()
        self.type_probabilities = {}
        self._calculate_weights()

    def _calculate_weights(self):
        """
        Calculates the probability for each feature type based on the pre-calculated
        'area_probability' column, re-normalized for the current feature set.
        """
        if self.features.empty:
            log.warning("Features GeoDataFrame is empty. Cannot calculate weights.")
            return

        if 'area_probability' not in self.features.columns:
            log.error("`area_probability` column not found. Please regenerate the master dataset.")
            return

        # Re-normalize the 'area_probability' for the features in this specific dataset item
        total_prob_in_subset = self.features['area_probability'].sum()
        if total_prob_in_subset > 0:
            self.features['renormalized_prob'] = self.features['area_probability']
        else:
            self.features['renormalized_prob'] = 0

        # Calculate the total probability for each feature type for sampling
        type_weights = self.features.groupby('feature_type')['renormalized_prob'].sum()

        self.type_probabilities = type_weights.to_dict()
        log.info(f"Calculated feature type probabilities for sampling: {self.type_probabilities}")

    def _generate_random_point_in_polygon(self, poly: Polygon) -> Point:
        """Generates a random point guaranteed to be within a given polygon."""
        min_x, min_y, max_x, max_y = poly.bounds
        while True:
            random_point = Point(random.uniform(min_x, max_x), random.uniform(min_y, max_y))
            if poly.contains(random_point):
                return random_point

    def generate_location(self) -> Point | None:
        """
        Generates and returns a single survivor location as a Shapely Point.

        Returns:
            Point | None: The generated location, or None if a location could not be generated.
        """
        if not self.type_probabilities:
            log.error("No feature type probabilities available. Cannot generate location.")
            return None

        # Step 1: Sample a feature type based on its re-normalized weight
        feature_types = list(self.type_probabilities.keys())
        probabilities = list(self.type_probabilities.values())
        chosen_type = random.choices(feature_types, weights=probabilities, k=1)[0]
        log.debug(f"Chosen feature type: {chosen_type}")

        # Step 2: Sample a specific feature from the chosen type, weighted by its re-normalized probability
        type_gdf = self.features[self.features['feature_type'] == chosen_type]
        if type_gdf.empty or type_gdf['renormalized_prob'].sum() == 0:
            log.warning(f"No valid features with re-normalized probability in type '{chosen_type}'.")
            return None 

        chosen_feature = type_gdf.sample(n=1, weights='renormalized_prob').iloc[0]

        # Step 3: Buffer the chosen feature and intersect with the main search area
        feature_geom = chosen_feature.geometry
        feature_buffer = feature_geom.buffer(30)
        
        # Create the main search circle geometry
        center_point = Point(self.dataset_item.center_point)
        center_gdf = gpd.GeoDataFrame(geometry=[center_point], crs="EPSG:4326")
        center_proj = center_gdf.to_crs(self.features.crs).geometry.iloc[0]
        main_search_circle = center_proj.buffer(self.dataset_item.radius_km * 1000)

        # The final area for sampling is the intersection of the two
        final_search_area = feature_buffer.intersection(main_search_circle)

        if final_search_area.is_empty:
             log.warning(f"Feature {chosen_feature.name} buffer is entirely outside the search radius. Skipping.")
             return None

        log.debug(f"Generating location within the intersection of the search circle and the 30m buffer of feature {chosen_feature.name}")
        return self._generate_random_point_in_polygon(final_search_area)
