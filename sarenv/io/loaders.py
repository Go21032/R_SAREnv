"""
This module provides classes for loading datasets generated by DataGenerator.
"""

import json
import os
from dataclasses import dataclass
from typing import Tuple

import geopandas as gpd
import numpy as np
import shapely

from sarenv.utils.logging_setup import get_logger

log = get_logger()


@dataclass
class SARDatasetItem:
    """
    A dataclass to hold the data for a single generated SAR environment size.

    Attributes:
        size (str): The name of the size (e.g., 'small', 'medium').
        center_point (tuple[float, float]): The (longitude, latitude) of the dataset's center.
        radius_km (float): The radius used to generate this dataset item.
        bounds (tuple[float, float, float, float]): The projected bounds (minx, miny, maxx, maxy) of the data.
        features (gpd.GeoDataFrame): A GeoDataFrame containing all geographic features.
        heatmap (np.ndarray): A 2D NumPy array representing the probability heatmap.
    """

    size: str
    center_point: tuple[float, float]
    radius_km: float
    bounds: Tuple[float, float, float, float]
    features: gpd.GeoDataFrame
    heatmap: np.ndarray


class DatasetLoader:
    """
    Loads a master SAR environment dataset and dynamically clips it to various sizes.

    This class is designed to load the 'master' files created by the
    DataGenerator's export_master_dataset method and then process them on-the-fly.
    """

    def __init__(self, dataset_directory: str):
        """
        Initializes the DynamicDatasetLoader.

        Args:
            dataset_directory (str): The path to the directory containing the master dataset files.
        """
        if not os.path.isdir(dataset_directory):
            log.error(f"Dataset directory not found at: {dataset_directory}")
            raise FileNotFoundError(
                f"The specified directory does not exist: {dataset_directory}"
            )

        self.dataset_directory = dataset_directory
        self.master_features_path = os.path.join(
            self.dataset_directory, "features_master.geojson"
        )
        self.master_heatmap_path = os.path.join(
            self.dataset_directory, "heatmap_master.npy"
        )

        if not os.path.exists(self.master_features_path):
            raise FileNotFoundError(
                f"Master features file not found: {self.master_features_path}"
            )
        if not os.path.exists(self.master_heatmap_path):
            raise FileNotFoundError(
                f"Master heatmap file not found: {self.master_heatmap_path}"
            )

        self.size_radii_km = {
            "small": 0.6,
            "medium": 1.8,
            "large": 3.2,
            "xlarge": 9.9,
        }

        # Cache for lazy-loading the master data
        self._master_features_gdf = None
        self._master_heatmap = None
        self._center_point = None
        self._projected_crs = None
        self._meter_per_bin = None
        self._bounds = None

    def _get_utm_epsg(self, lon: float, lat: float) -> str:
        """Calculates the appropriate UTM zone EPSG code for a given point."""
        zone = int((lon + 180) / 6) + 1
        return f"326{zone}" if lat >= 0 else f"327{zone}"

    def _load_master_data_if_needed(self):
        """Loads the master GeoJSON and heatmap from disk if not already in cache."""
        if self._master_features_gdf is not None:
            return

        log.info("Loading master dataset from disk for the first time...")
        try:
            with open(self.master_features_path, "r") as f:
                geojson_data = json.load(f)

            # --- Load Metadata ---
            self._center_point = tuple(geojson_data["center_point"])
            self._meter_per_bin = geojson_data["meter_per_bin"]
            # --- NEW: Load master bounds from metadata ---
            self._bounds = tuple(geojson_data["bounds"])
            log.info(
                f"Loaded metadata: center={self._center_point}, resolution={self._meter_per_bin} m/bin"
            )

            # --- Determine Projected CRS ---
            self._projected_crs = f"EPSG:{self._get_utm_epsg(self._center_point[0], self._center_point[1])}"

            # --- Load Main Data ---
            self._master_features_gdf = gpd.GeoDataFrame.from_features(
                geojson_data["features"], crs="EPSG:4326"
            )
            self._master_heatmap = np.load(self.master_heatmap_path)
            log.info("Successfully loaded and cached master dataset.")

        except KeyError as e:
            log.error(
                f"Could not find required key '{e}' in {self.master_features_path}. The file might be corrupt or incomplete."
            )
            self._master_features_gdf = None  # Invalidate cache
        except Exception as e:
            log.error(
                f"An error occurred while loading the master dataset: {e}",
                exc_info=True,
            )
            self._master_features_gdf = None  # Invalidate cache

    def get_available_sizes(self) -> list[str]:
        """Returns a list of all predefined size names."""
        return list(self.size_radii_km.keys())

    def _world_to_image_for_master(
        self, x_world: np.ndarray, y_world: np.ndarray
    ) -> tuple[np.ndarray, np.ndarray]:
        """Converts world coordinates (projected) to pixel coordinates of the master heatmap."""
        # --- UPDATED: Use the loaded master bounds for conversion ---
        master_minx, master_miny, _, _ = self._bounds
        x_img = (x_world - master_minx) / self._meter_per_bin
        y_img = (y_world - master_miny) / self._meter_per_bin
        return x_img.astype(int), y_img.astype(int)

    def load_size(self, size: str) -> SARDatasetItem | None:
        """
        Loads the master data and clips it to the specified size.

        Args:
            size (str): The name of the size to load (e.g., 'medium').

        Returns:
            SARDatasetItem | None: A dataclass instance containing the clipped/cropped data.
        """
        log.info(f"Attempting to generate dataset for size: '{size}'")
        if size not in self.size_radii_km:
            log.error(
                f"Size '{size}' is not a valid size name. Available sizes: {self.get_available_sizes()}"
            )
            return None

        self._load_master_data_if_needed()

        if self._master_features_gdf is None:
            log.error(
                "Cannot process size request because the master dataset failed to load."
            )
            return None

        radius_km = self.size_radii_km[size]
        # Project master features to the projected CRS for accurate spatial operations
        self._master_features_gdf.to_crs(self._projected_crs, inplace=True)
        # Use master data directly for the largest size to avoid unnecessary clipping
        if size == "xlarge":
            log.info(
                "Requested size 'xlarge' matches master dataset. Returning full data."
            )
            # Calculate area in square meters from radius_km
            normalized_heatmap = self._master_heatmap.copy() / sum(self._master_heatmap.flatten())
            assert np.isclose(normalized_heatmap.sum(), 1.0, atol=1e-6), "Normalized heatmap does not sum to 1"
            return SARDatasetItem(
                size=size,
                center_point=self._center_point,
                radius_km=radius_km,
                bounds=self._bounds,  # Use the master bounds directly
                features=self._master_features_gdf.copy(),
                heatmap=normalized_heatmap,
            )

        # Create clipping geometry
        clipping_point_wgs84 = gpd.GeoDataFrame(
            geometry=[shapely.Point(self._center_point)], crs="EPSG:4326"
        )
        clipping_point_proj = clipping_point_wgs84.to_crs(self._projected_crs)
        clipping_circle_proj = clipping_point_proj.buffer(radius_km * 1000).iloc[0]
        clipped_bounds = clipping_circle_proj.bounds

        # Clip features
        clipped_features_proj = gpd.clip(
            self._master_features_gdf, clipping_circle_proj
        )
        log.info(
            f"Clipped features to {len(clipped_features_proj)} items for size '{size}'."
        )

        # Crop heatmap using the calculated bounds
        min_x_w, min_y_w, max_x_w, max_y_w = clipped_bounds

        img_min_x, img_min_y = self._world_to_image_for_master(
            np.array([min_x_w]), np.array([min_y_w])
        )
        img_max_x, img_max_y = self._world_to_image_for_master(
            np.array([max_x_w]), np.array([max_y_w])
        )

        # Clamp values to be within the master heatmap dimensions
        h, w = self._master_heatmap.shape
        img_min_y, img_max_y = np.clip([img_min_y[0], img_max_y[0]], 0, h)
        img_min_x, img_max_x = np.clip([img_min_x[0], img_max_x[0]], 0, w)
        # Create a mask for the disc-shaped region within the cropped heatmap
        yy, xx = np.ogrid[img_min_y:img_max_y, img_min_x:img_max_x]
        center_x = (clipping_point_proj.geometry.x.values[0] - self._bounds[0]) / self._meter_per_bin
        center_y = (clipping_point_proj.geometry.y.values[0] - self._bounds[1]) / self._meter_per_bin
        mask = ((xx - center_x) ** 2 + (yy - center_y) ** 2) <= (radius_km * 1000 / self._meter_per_bin) ** 2

        cropped_heatmap = self._master_heatmap[img_min_y:img_max_y, img_min_x:img_max_x]
        cropped_heatmap = np.where(mask, cropped_heatmap, 0)
        log.info(f"Cropped heatmap to shape {cropped_heatmap.shape} for size '{size}'.")
        normalized_heatmap = cropped_heatmap / sum(cropped_heatmap.flatten())
        assert np.isclose(normalized_heatmap.sum(), 1.0, atol=1e-6), "Normalized heatmap does not sum to 1"
        return SARDatasetItem(
            size=size,
            center_point=self._center_point,
            radius_km=radius_km,
            bounds=clipped_bounds,  # Pass the calculated bounds
            features=clipped_features_proj,
            heatmap=normalized_heatmap,
        )

    def load_all(self) -> dict[str, SARDatasetItem]:
        """
        Loads and processes data for all available sizes.

        Returns:
            dict[str, SARDatasetItem]: A dictionary where keys are size names
                                       and values are the corresponding SARDatasetItem objects.
        """
        log.info("Loading all available sizes from the master dataset.")
        all_data = {}
        for size in self.get_available_sizes():
            item = self.load_size(size)
            if item:
                all_data[size] = item
        return all_data

