"""
This module provides classes for loading datasets generated by DataGenerator.
"""

import json
import os
from dataclasses import dataclass
from typing import Tuple

import geopandas as gpd
import numpy as np
import shapely

from sarenv.utils.logging_setup import get_logger
from sarenv.utils.lost_person_behavior import get_environment_radius_by_size, get_available_sizes
log = get_logger()


@dataclass
class SARDatasetItem:
    """
    A dataclass to hold the data for a single generated SAR environment size.

    Attributes:
        size (str): The name of the size (e.g., 'small', 'medium').
        center_point (tuple[float, float]): The (longitude, latitude) of the dataset's center.
        radius_km (float): The radius used to generate this dataset item.
        bounds (tuple[float, float, float, float]): The projected bounds (minx, miny, maxx, maxy) of the data.
        features (gpd.GeoDataFrame): A GeoDataFrame containing all geographic features.
        heatmap (np.ndarray): A 2D NumPy array representing the probability heatmap. Its sum reflects the probability mass within this clip.
    """

    size: str
    center_point: tuple[float, float]
    radius_km: float
    bounds: Tuple[float, float, float, float]
    features: gpd.GeoDataFrame
    heatmap: np.ndarray
    environment_climate: str
    environment_type: str


class DatasetLoader:
    """
    Loads a master SAR environment dataset and dynamically clips it to various sizes.

    This class is designed to load the 'master' files created by the
    DataGenerator's export_dataset method and then process them on-the-fly.
    """

    def __init__(self, dataset_directory: str):
        """
        Initializes the DynamicDatasetLoader.

        Args:
            dataset_directory (str): The path to the directory containing the master dataset files.
        """
        if not os.path.isdir(dataset_directory):
            log.error(f"Dataset directory not found at: {dataset_directory}")
            raise FileNotFoundError(
                f"The specified directory does not exist: {dataset_directory}"
            )

        self.dataset_directory = dataset_directory
        self.master_features_path = os.path.join(
            self.dataset_directory, "features.geojson"
        )
        self.master_heatmap_path = os.path.join(self.dataset_directory, "heatmap.npy")

        if not os.path.exists(self.master_features_path):
            raise FileNotFoundError(
                f"Master features file not found: {self.master_features_path}"
            )
        if not os.path.exists(self.master_heatmap_path):
            raise FileNotFoundError(
                f"Master heatmap file not found: {self.master_heatmap_path}"
            )

        # Cache for lazy-loading the master data
        self._master_features_gdf = None
        self._master_features_gdf_proj = None
        self._master_probability_map = None
        self._center_point = None
        self._projected_crs = None
        self._meter_per_bin = None
        self._bounds = None
        self._climate = None
        self._environment_type = None

    def _get_utm_epsg(self, lon: float, lat: float) -> str:
        """Calculates the appropriate UTM zone EPSG code for a given point."""
        zone = int((lon + 180) / 6) + 1
        return f"326{zone}" if lat >= 0 else f"327{zone}"

    def _load_master_data_if_needed(self):
        """Loads the master GeoJSON and probability map from disk if not already in cache."""
        if self._master_probability_map is not None:
            return

        log.info("Loading master dataset from disk for the first time...")
        try:
            with open(self.master_features_path, "r") as f:
                geojson_data = json.load(f)

            # --- Load Metadata ---
            self._center_point = tuple(geojson_data["center_point"])
            self._meter_per_bin = geojson_data["meter_per_bin"]
            self._bounds = tuple(geojson_data["bounds"])
            self._climate = geojson_data["climate"]
            self._environment_type = geojson_data["environment_type"]
            log.info(
                f"Loaded metadata: center={self._center_point}, resolution={self._meter_per_bin} m/bin"
            )

            # --- Determine Projected CRS ---
            self._projected_crs = f"EPSG:{self._get_utm_epsg(self._center_point[0], self._center_point[1])}"

            # --- Load Main Data ---
            self._master_features_gdf = gpd.GeoDataFrame.from_features(
                geojson_data["features"], crs="EPSG:4326"
            )
            self._master_features_gdf_proj = self._master_features_gdf.to_crs(
                self._projected_crs
            )
            # Load the pre-combined and pre-normalized probability map
            self._master_probability_map = np.load(self.master_heatmap_path)
            log.info(
                f"Loaded master probability map with shape {self._master_probability_map.shape}"
            )

        except KeyError as e:
            log.error(
                f"Could not find required key '{e}' in {self.master_features_path}. The file might be corrupt or incomplete."
            )
            raise
        except Exception as e:
            log.error(
                f"An error occurred while loading the master dataset: {e}",
                exc_info=True,
            )
            raise


    def _world_to_image_for_master(
        self, x_world: np.ndarray, y_world: np.ndarray
    ) -> tuple[np.ndarray, np.ndarray]:
        """Converts world coordinates (projected) to pixel coordinates of the master heatmap."""
        master_minx, master_miny, _, _ = self._bounds
        x_img = (x_world - master_minx) / self._meter_per_bin
        y_img = (y_world - master_miny) / self._meter_per_bin
        return x_img.astype(int), y_img.astype(int)

    def load_environment(self, size: str) -> SARDatasetItem | None:
        """
        Loads the master data and clips it to a specified size. The resulting
        heatmap is a crop of the master probability map, and its sum will reflect
        its portion of the total probability.

        Args:
            size (str): The name of the size to load (e.g., 'medium').

        Returns:
            SARDatasetItem | None: A dataclass instance containing the clipped/cropped data.
        """
        log.info(f"Attempting to generate dataset for size: '{size}'")
        self._load_master_data_if_needed()

        if self._master_probability_map is None:
            log.error(
                "Cannot process size request because the master probability map failed to load."
            )
            return None

        radius_km = get_environment_radius_by_size(
            self._environment_type, self._climate, size
        )

        # Create clipping geometry for features
        clipping_point_wgs84 = gpd.GeoDataFrame(
            geometry=[shapely.Point(self._center_point)], crs="EPSG:4326"
        )
        clipping_point_proj = clipping_point_wgs84.to_crs(self._projected_crs)
        clipping_circle_proj = clipping_point_proj.buffer(radius_km * 1000).iloc[0]
        clipped_bounds = clipping_circle_proj.bounds

        clipped_features_proj = gpd.clip(
            self._master_features_gdf_proj, clipping_circle_proj
        )
        log.info(
            f"Clipped features to {len(clipped_features_proj)} items for size '{size}'."
        )

        # Get pixel boundaries for cropping the heatmap
        min_x_w, min_y_w, max_x_w, max_y_w = clipped_bounds
        img_min_x, img_min_y = self._world_to_image_for_master(
            np.array([min_x_w]), np.array([min_y_w])
        )
        img_max_x, img_max_y = self._world_to_image_for_master(
            np.array([max_x_w]), np.array([max_y_w])
        )

        h, w = self._master_probability_map.shape
        img_min_y, img_max_y = np.clip([img_min_y[0], img_max_y[0]], 0, h)
        img_min_x, img_max_x = np.clip([img_min_x[0], img_max_x[0]], 0, w)

        # Crop the master probability map
        cropped_map = self._master_probability_map[
            img_min_y:img_max_y, img_min_x:img_max_x
        ]

        # Create a circular mask for the cropped area to ensure it's not a square
        yy, xx = np.mgrid[img_min_y:img_max_y, img_min_x:img_max_x]
        center_x_master_px = (
            clipping_point_proj.geometry.x.values[0] - self._bounds[0]
        ) / self._meter_per_bin
        center_y_master_px = (
            clipping_point_proj.geometry.y.values[0] - self._bounds[1]
        ) / self._meter_per_bin
        radius_px = (radius_km * 1000) / self._meter_per_bin
        mask = (
            (xx - center_x_master_px) ** 2 + (yy - center_y_master_px) ** 2
        ) <= radius_px**2

        # Apply mask. The resulting map is NOT re-normalized.
        # Its sum reflects its portion of the total probability.
        final_heatmap = np.where(mask, cropped_map, 0)
        log.info(
            f"Final heatmap for size '{size}' has a probability sum of {np.sum(final_heatmap):.4f}"
        )

        return SARDatasetItem(
            size=size,
            center_point=self._center_point,
            radius_km=radius_km,
            bounds=clipped_bounds,
            features=clipped_features_proj,
            heatmap=final_heatmap,
            environment_climate=self._climate,
            environment_type=self._environment_type,
        )

    def load_all(self) -> dict[str, SARDatasetItem]:
        """
        Loads and processes data for all available sizes.

        Returns:
            dict[str, SARDatasetItem]: A dictionary where keys are size names
                                       and values are the corresponding SARDatasetItem objects.
        """
        log.info("Loading all available sizes from the master dataset.")
        all_data = {}
        for size in get_available_sizes():
            item = self.load_environment(size)
            if item:
                all_data[size] = item
        return all_data
