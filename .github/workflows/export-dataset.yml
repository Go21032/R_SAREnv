name: Export SAR Dataset

on:
  # Manual trigger
  workflow_dispatch:
  
  # Trigger on push to main branch (optional)
  push:
    branches:
      - main
    paths:
      - 'data/export_all_data.py'
      - 'sarenv/**'
  
  # Trigger on release creation (optional)
  release:
    types: [created]

env:
  PYTHON_VERSION: '3.11'

jobs:
  export-dataset:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          gdal-bin \
          libgdal-dev \
          libspatialindex-dev \
          libproj-dev \
          proj-data \
          proj-bin \
          libgeos-dev
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .
        # Install testing dependencies
        pip install pytest pytest-cov pytest-mock
    
    - name: Run unit tests
      run: |
        echo "Running unit tests..."
        python -m pytest tests/ -v --tb=short --cov=. --cov-report=term-missing
      continue-on-error: true
    
    - name: Create cache directory
      run: mkdir -p cache
    

    
    - name: Run dataset export
      run: |
        cd data
        python export_all_data.py
    
    - name: Create timestamp
      id: timestamp
      run: echo "timestamp=$(date +'%Y%m%d_%H%M%S')" >> $GITHUB_OUTPUT
    
    - name: Archive dataset
      run: |
        # Create a compressed archive of the generated dataset
        tar -czf sar-dataset-${{ steps.timestamp.outputs.timestamp }}.tar.gz \
          --exclude='*.git*' \
          --exclude='__pycache__' \
          --exclude='*.pyc' \
          sarenv_dataset/
        
        # Create a zip file as well for easier access
        zip -r sar-dataset-${{ steps.timestamp.outputs.timestamp }}.zip \
          sarenv_dataset/ \
          -x "*.git*" "*__pycache__*" "*.pyc"
        
        # Create checksums
        sha256sum sar-dataset-${{ steps.timestamp.outputs.timestamp }}.tar.gz > checksums.txt
        sha256sum sar-dataset-${{ steps.timestamp.outputs.timestamp }}.zip >> checksums.txt
        
        # Display archive info
        echo "Archive created:"
        ls -lh sar-dataset-${{ steps.timestamp.outputs.timestamp }}.*
        echo "Using default dataset settings"
        echo "Number of locations: $(find sarenv_dataset -name "*.json" | wc -l)"
    
    - name: Upload dataset as artifact
      uses: actions/upload-artifact@v4
      with:
        name: sar-dataset-${{ steps.timestamp.outputs.timestamp }}
        path: |
          sar-dataset-${{ steps.timestamp.outputs.timestamp }}.tar.gz
          sar-dataset-${{ steps.timestamp.outputs.timestamp }}.zip
          checksums.txt
        retention-days: 90
        compression-level: 0  # Already compressed
    

    
    - name: Summary
      run: |
        echo "## Workflow Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Dataset Settings:** Default (library defaults)" >> $GITHUB_STEP_SUMMARY
        echo "- **Export Time:** ${{ steps.timestamp.outputs.timestamp }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Archive Size:** $(du -h sar-dataset-${{ steps.timestamp.outputs.timestamp }}.tar.gz | cut -f1)" >> $GITHUB_STEP_SUMMARY
        echo "- **Locations Processed:** $(find sarenv_dataset -name "*.json" | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- **Artifact:** [sar-dataset-${{ steps.timestamp.outputs.timestamp }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“¥ Download Options:" >> $GITHUB_STEP_SUMMARY
        echo "1. **GitHub Actions Artifacts** (90-day retention) - Available immediately after workflow completion" >> $GITHUB_STEP_SUMMARY
